{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 개요\n",
    "이 코드 피일은 설정된 키워드를 기반으로 해당 키워드들이 사용된 게시글들을 DC인사이드 내에서 크롤링하여 제목과 본문을 csv형식으로 저장해주는 역할을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests \n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 검색 키워드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DC인사이드에서 검색하고자 하는 키워드를 아래 Array에 넣습니다. 추가한 단어들에 대해서 DC인사이드에서 최근 게시글 순으로 최대량을 검색하여 수집을 시작합니다.\"\"\"\n",
    "keywords = ['최저임금','최저시급', '최저 임금', '최저 시급']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 검색어를 이용한 페이지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"앞서 입력한 키워드들마다, DC인사이드 내 검색 최대 페이지인 120번까지 검색을 진행합니다. 검색된 결과는 'pages' 배열에 (제목, URL)의 형태로 저장됩니다.\"\"\"\n",
    "\n",
    "pages = []\n",
    "for keyword in keywords:\n",
    "    print(keyword)\n",
    "    for page_num in tqdm(range(1,121)):\n",
    "        ## 페이지의 검색 메인 URL 입니다. \n",
    "        BASE_URL = \"https://search.dcinside.com/post/p/%s/sort/latest/q/%s\"%(str(page_num),keyword)\n",
    "        headers = {\"User-Agent\": \"owen (Android; Yusik_fuckyou)\", \"Cookie\": \"list_count=100\"}\n",
    "        \n",
    "        # 검색창에 키워드를 입력하고, 각 검색페이지 번호를 돌며 문서들을 수집합니다.\n",
    "        params = {}\n",
    "        response = requests.get(BASE_URL, params=params ,headers=headers) \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        #웹사이트의 경우 수시로 url이나 HTML 구조를 변경할 수 있으니, 유의하시기 바랍니다. 현재의 경우 html 구조 내에서 sch_result_list내에 검색된 정보를 저장하고 있습니다.\n",
    "        html_list = soup.find(class_='sch_result_list').find_all('li')\n",
    "        for i in html_list: \n",
    "            title_list = i.find('a', href=True).text # 제목 \n",
    "            URL_list = i.find('a', href=True)['href'] # URL 주소 \n",
    "            pages.append((title_list , URL_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 검색된 페이지 내의 내용 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"키워드를 통해 수집된 모든 페이지들을 돌며 해당 링크에 존재하는 게시글의 정보를 수집합니다.\"\"\"\n",
    "\n",
    "new_pages = []\n",
    "for page in tqdm(pages[:40]):\n",
    "    headers = {\"User-Agent\": \"owen (Android; RandomID)\", \"Cookie\": \"list_count=100\"}\n",
    "    try: # request 에러로 검색이 실패하는 경우가 종종 있어 try문을 이용해 실패시에도 한번 더 요청하도록 구현합니다.\n",
    "        response = requests.get(page[1], headers=headers) \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        \"\"\"11월 14일차 기준 본문의 경우 'script' 클래스였으나, HTML 구조의 변경으로 코드 재사용시 이 부분을 확인해야 합니다.\"\"\"\n",
    "        new_pages.append((page[0],soup.find('script').get_text()[17:-2]))\n",
    "    except:\n",
    "        try:\n",
    "            response = requests.get(page[1], headers=headers) \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            new_pages.append((page[0],soup.find('script').get_text()[17:-2]))\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"앞서 수집한 페이지들을 순회하며 본문 내용을 [제목, 본문, 주소] 형태로 정리하여 재수집을 진행합니다. 수집된 데이터는 최종적으로 datas 배열에 저장됩니다.\"\"\"\n",
    "\n",
    "datas = []\n",
    "for page in tqdm(new_pages):\n",
    "    # 헤더 설정 \n",
    "    headers = {\"User-Agent\": \"owen (Android; RandomID)\", \"Cookie\": \"list_count=100\"}\n",
    "    # html \n",
    "    try: # request 에러로 검색이 실패하는 경우가 종종 있어 try문을 이용해 실패시에도 한번 더 요청하도록 구현합니다.\n",
    "        response = requests.get(page[1], headers=headers) \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')    \n",
    "        \n",
    "        \"\"\"위와 마찬가지로 11월 14일차 기준 본문의 경우 'thum-txtin' 클래스였으나, HTML 구조의 변경으로 코드 재사용시 이 부분을 확인해야 합니다.\"\"\"\n",
    "        text_lines = soup.find(class_='thum-txtin').text\n",
    "    except:\n",
    "        try:\n",
    "            response = requests.get(page[1], headers=headers[0]) \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')    \n",
    "            text_lines = soup.find('div',class_='thum-txtin').find_all('p')\n",
    "        except:\n",
    "            continue\n",
    "    datas.append((page[0],text_lines,page[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 수집된 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"위 코드들을 통해 수집된 DC인사이드 문서들을 전체글과 url 주소를 짝지어 CSV파일로 저장합니다.\"\"\"\n",
    "d = pd.DataFrame({'fulltext':[d[0]+d[1] for d in datas],\n",
    "                 'url':[d[2] for d in datas]})\n",
    "\n",
    "# 중복 되거나 누락된 글들은 필터링 됩니다.                 \n",
    "data = data.dropna()\n",
    "d = d.drop_duplicates(subset=['fulltext'], keep='first')\n",
    "\n",
    "# csv 파일로 저장합니다.\n",
    "d.to_csv('dcinside_search(%s).csv'%\",\".join(['\"'+key+'\"' for key in keywords]),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
